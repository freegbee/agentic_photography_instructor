name: ${NAME}
services:
  data-service:
    build:
      context: image-acquisition-service
      dockerfile: Dockerfile
    container_name: image-acquisition-service
    environment:
      # ENV_NAME=docker-Eintrag sorgt dafür, dass die os.environment Variable gesetzt wird. (Verwendung vom docker.yaml)
      - ENV_NAME=docker
      # PYTHONPATH=/app/src-Eintrag sorgt dafür, dass Python Module(-Import) relativ zum /src-Verzeichnis finden kann
      - PYTHONPATH=/app/src
    volumes:
      - resources:/app/volumes/resources
      - ../src:/app/src
      - ../configs:/app/configs
    networks:
      - ml-network

  juror-service:
    build:
      context: juror-service
      dockerfile: Dockerfile
    container_name: juror-service
    ports:
      - "${JUROR_PORT}:${JUROR_PORT}"
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
    command: >
      python -m uvicorn juror_server.JurorServer:app
      --host 0.0.0.0
      --port ${JUROR_PORT}
      --log-level info
    volumes:
      - ../src/:/app
    networks:
      - ml-network
    # Folgender Block sorgt dafür, dass der Container auf eine GPU mit CUDA zugreifen kann.
    # Offen, ob das auf dem Mac stört und wir profile brauchen, oder nicht
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

  train:
    build:
      context: train
      dockerfile: Dockerfile
    container_name: train
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app/src
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
    volumes:
      - resources:/app/volumes/resources
      - ../src:/app/src
      - ../configs:/app/configs
      - ../test:/app/test
    depends_on:
      - mlflow
    networks:
      - ml-network

  mlflow:
    build:
      context: mlflow
      dockerfile: Dockerfile
    container_name: mlflow
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    volumes:
      # Artefakte (Runs) persistent speichern
      - mlruns:/mlruns
      # Backend-DB persistent speichern
      - mlflow:/mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port ${MLFLOW_PORT}
      --backend-store-uri ${MLFLOW_BACKEND_STORE_URI}
      --default-artifact-root ${MLFLOW_ARTIFACT_ROOT}
      --allowed-hosts "*"
    networks:
      - ml-network

  prometheus:
    build:
      context: prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    ports:
      - "${MONITORING_PROMETHEUS_PORT}:9090"
    environment:
      JUROR_PORT: "${JUROR_PORT}"
      MLFLOW_PORT: "${MLFLOW_PORT}"
    volumes:
      - prometheus_data:/prometheus
      - ./prometheus/prometheus.yml.template:/etc/prometheus/prometheus.yml.template:ro
    networks:
      - ml-network

  grafana:
    build:
      context: grafana
      dockerfile: Dockerfile
    container_name: grafana
    ports:
      - "${MONITORING_GRAFANA_PORT}:3000"
    environment:
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - ml-network
    depends_on:
      - prometheus

volumes:
  test:
    external: true
  mlflow:
    name: mlflow
    driver: local
  mlruns:
    name: mlruns
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  resources:
    name: resources
    driver: local

networks:
  ml-network:
    driver: bridge