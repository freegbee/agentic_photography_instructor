name: ${NAME}
services:
  data-service:
    build:
      context: image-acquisition-service
      dockerfile: Dockerfile
    container_name: image-acquisition-service
    environment:
      # ENV_NAME=docker-Eintrag sorgt dafür, dass die os.environment Variable gesetzt wird. (Verwendung vom docker.yaml)
      - ENV_NAME=docker
      # PYTHONPATH=/app/src-Eintrag sorgt dafür, dass Python Module(-Import) relativ zum /src-Verzeichnis finden kann
      - PYTHONPATH=/app/src
    volumes:
      - resources:/app/volumes/resources
      - ../src:/app/src
      - ../configs:/app/configs
    networks:
      - ml-network

  juror-service:
    build:
      context: juror-service
      dockerfile: Dockerfile
    container_name: juror-service
    ports:
      - "${JUROR_PORT}:${JUROR_PORT}"
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
    command: >
      python -m uvicorn juror_server.JurorServer:app
      --host 0.0.0.0
      --port ${JUROR_PORT}
      --log-level trace
    volumes:
      - ../src/:/app
    networks:
      - ml-network
    # Folgender Block sorgt dafür, dass der Container auf eine GPU mit CUDA zugreifen kann.
    # Offen, ob das auf dem Mac stört und wir profile brauchen, oder nicht
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

  train:
    build:
      context: train
      dockerfile: Dockerfile
    container_name: train
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app/src
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
    volumes:
      - resources:/app/volumes/resources
      - ../src:/app/src
      - ../configs:/app/configs
      - ../test:/app/test
    depends_on:
      - mlflow
    networks:
      - ml-network

  mlflow:
    build:
      context: mlflow
      dockerfile: Dockerfile
    container_name: mlflow
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    volumes:
      # Artefakte (Runs) persistent speichern
      - mlruns:/mlruns
      # Backend-DB persistent speichern
      - mlflow:/mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port ${MLFLOW_PORT}
      --backend-store-uri ${MLFLOW_BACKEND_STORE_URI}
      --default-artifact-root ${MLFLOW_ARTIFACT_ROOT}
      --allowed-hosts "*"
    networks:
      - ml-network

volumes:
  test:
    external: true
  mlflow:
    name: mlflow
    driver: local
  mlruns:
    name: mlruns
    driver: local
  resources:
    name: resources
    driver: local

networks:
  ml-network:
    driver: bridge