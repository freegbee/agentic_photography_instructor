name: ${NAME}
services:
  notebook:
    build:
      context: notebook
      dockerfile: Dockerfile
    container_name: notebook
    ports:
      - "${NOTEBOOK_PORT:-8888}:${NOTEBOOK_PORT:-8888}"
    environment:
      - PYTHONPATH=/app/src
      - NOTEBOOK_PORT=${NOTEBOOK_PORT}
      - MLFLOW_PORT=${MLFLOW_PORT}
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
      - MLFLOW_ARTIFACT_ROOT=/mlruns
      - JUPYTER_TOKEN=${JUPYTER_TOKEN}
    volumes:
      - ../notebooks:/app/notebooks
      - ../src:/app/src
      - ../configs:/app/configs
      - resources:${IMAGE_VOLUME_PATH}
      # WICHTIG: Artefakt-Volume auch im Notebook mounten, falls der Client lokal auf Dateisystem zugreift
      - mlruns:/mlruns
    command: [
      "tini", "--",
      "jupyter", "lab",
      "--ip=0.0.0.0",
      "--port", "${NOTEBOOK_PORT:-8888}",
      "--no-browser",
      "--IdentityProvider.token=${JUPYTER_TOKEN}",
      "--ServerApp.root_dir=/app/notebooks",
      "--allow-root"
    ]
    depends_on:
      - mlflow
    networks:
      - ml-network

  experiment-service:
    build:
      context: experiment-service
      dockerfile: Dockerfile
    container_name: experiment-service
    # Container laufen lassen und auf Eingaben warten, auch wenn kein Terminal verbunden ist
    stdin_open: true
    tty: true
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
      - IMAGE_VOLUME_PATH=${IMAGE_VOLUME_PATH}
      - IMAGE_ACQUISITION_SERVICE_URL=http://image-acquisition-service:${IMAGE_ACQUISITION_PORT}
      - JUROR_SERVICE_URL=http://juror-service:${JUROR_PORT}
      - HF_HOME=/app/volumes/model_caches/huggingface
      - TORCH_HOME=/app/volumes/model_caches/torch
    volumes:
      - ../src/:/app
      - resources:${IMAGE_VOLUME_PATH}
      - ../configs:/app/configs
      - mlruns:/mlruns
      - model_caches:/app/volumes/model_caches
    depends_on:
      - prometheus
      - mlflow
      - image-acquisition-service
      - juror-service
    networks:
      - ml-network

  image-acquisition-service:
    build:
      context: image-acquisition-service
      dockerfile: Dockerfile
    container_name: image-acquisition-service
    ports:
      - "${IMAGE_ACQUISITION_PORT}:${IMAGE_ACQUISITION_PORT}"
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
      - IMAGE_VOLUME_PATH=${IMAGE_VOLUME_PATH}
      - KAGGLE_USERNAME=${KAGGLE_USERNAME}
      - KAGGLE_KEY=${KAGGLE_KEY}
    command: >
      python -m uvicorn image_acquisition.acquisition_server.AcquisitionServer:app
      --host 0.0.0.0
      --port ${IMAGE_ACQUISITION_PORT}
      --log-level info
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:${IMAGE_ACQUISITION_PORT}/health || exit 1" ]
      interval: 30s
      timeout: 5s
      retries: 3
    volumes:
      - resources:${IMAGE_VOLUME_PATH}
      - ../src/:/app
      - ../configs:/app/configs
    depends_on:
      - prometheus
    networks:
      - ml-network

  juror-service:
    build:
      context: juror-service
      dockerfile: Dockerfile
    container_name: juror-service
    platform: linux/arm64  # Force ARM64 for Apple Silicon
    ports:
      - "${JUROR_PORT}:${JUROR_PORT}"
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
      - HF_HOME=/app/volumes/model_caches/huggingface
      - TORCH_HOME=/app/volumes/model_caches/torch
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - MPS_AVAILABLE=1
    command: >
      python -m uvicorn juror_server.JurorServer:app
      --host 0.0.0.0
      --port ${JUROR_PORT}
      --log-level info
      --workers ${JUROR_WORKERS}
    volumes:
      - ../src/:/app
      - model_caches:/app/volumes/model_caches
    depends_on:
      - prometheus
    networks:
      - ml-network
    shm_size: 2gb
    # deploy block removed: moved to docker/docker-compose.windows.override.yml
    # If you want to enable the GPU reservation (Windows), start compose with:
    # docker-compose -f docker-compose.yml -f docker/docker-compose.windows.override.yml up

  train:
    build:
      context: train
      dockerfile: Dockerfile
    container_name: train
    stdin_open: true
    tty: true
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app/src
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
      - IMAGE_VOLUME_PATH=${IMAGE_VOLUME_PATH}
      - IMAGE_ACQUISITION_SERVICE_URL=http://image-acquisition-service:${IMAGE_ACQUISITION_PORT}
      - JUROR_SERVICE_URL=http://juror-service:${JUROR_PORT}
      - MLFLOW_ARTIFACT_ROOT=${MLFLOW_ARTIFACT_ROOT}
      - HF_HOME=/app/volumes/model_caches/huggingface
      - TORCH_HOME=/app/volumes/model_caches/torch
    volumes:
      - resources:${IMAGE_VOLUME_PATH}
      - ../src:/app/src
      - ../configs:/app/configs
      - ../test:/app/test
      - mlruns:/mlruns
      - model_caches:/app/volumes/model_caches
    depends_on:
      - mlflow
      - juror-service
      - image-acquisition-service
    networks:
      - ml-network

  mlflow:
    build:
      context: mlflow
      dockerfile: Dockerfile
    container_name: mlflow
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    volumes:
      # Artefakte (Runs) persistent speichern
      - mlruns:/mlruns
      # Backend-DB persistent speichern
      - mlflow:/mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port ${MLFLOW_PORT}
      --backend-store-uri ${MLFLOW_BACKEND_STORE_URI}
      --default-artifact-root ${MLFLOW_ARTIFACT_ROOT}
      --allowed-hosts "*"
      --serve-artifacts
    networks:
      - ml-network

  prometheus:
    build:
      context: prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    ports:
      - "${MONITORING_PROMETHEUS_PORT}:${MONITORING_PROMETHEUS_PORT}"
    environment:
      IMAGE_ACQUISITION_PORT: "${IMAGE_ACQUISITION_PORT}"
      JUROR_PORT: "${JUROR_PORT}"
      MLFLOW_PORT: "${MLFLOW_PORT}"
      MONITORING_PROMETHEUS_PORT: "${MONITORING_PROMETHEUS_PORT}"
      MONITORING_PROMETHEUS_GPU_EXPORTER_PORT: "${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}"
    volumes:
      - prometheus_data:/prometheus
      - ./prometheus/prometheus.yml.template:/etc/prometheus/prometheus.yml.template:ro
    networks:
      - ml-network

  grafana:
    build:
      context: grafana
      dockerfile: Dockerfile
    container_name: grafana
    ports:
      - "${MONITORING_GRAFANA_PORT}:${MONITORING_GRAFANA_PORT}"
    environment:
      MONITORING_GRAFANA_PORT: "${MONITORING_GRAFANA_PORT}"
      GF_SERVER_HTTP_PORT: "${MONITORING_GRAFANA_PORT}"
      GF_SERVER_ROOT_URL: "http://localhost:${MONITORING_GRAFANA_PORT}"
      GF_USERS_ALLOW_SIGN_UP: "false"
      PROMETHEUS_URL: "http://prometheus:${MONITORING_PROMETHEUS_PORT}"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - ml-network
    depends_on:
      - prometheus

  system-gpu-exporter:
    build:
      context: ./exporter
      dockerfile: Dockerfile
    container_name: system-gpu-exporter
    ports:
      - "${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}:${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}"
    environment:
      - ENV_NAME=docker
      - SCRAPE_INTERVAL_S=5
      - EXPORTER_PORT=8000
    volumes:
      - ../src/:/app
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}/metrics || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
    # If you want to expose host GPUs to this container (NVIDIA), ensure the host has NVIDIA Container Toolkit
    # and your Docker/Compose supports device_requests. Uncomment device_requests to request all gpus.
    #device_requests:
    #  - driver: "nvidia"
    #    count: -1
    #    capabilities: ["gpu"]
    networks:
      - ml-network

  annotations-browser:
    # reuse the build context so dependencies are the same as image-acquisition-service
    build:
      context: image-acquisition-service
      dockerfile: Dockerfile
    container_name: annotations-browser
    ports:
      - "${ANNOTATIONS_BROWSER_PORT}:${ANNOTATIONS_BROWSER_PORT}"
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
      - IMAGE_VOLUME_PATH=${IMAGE_VOLUME_PATH}
      - ANNOTATIONS_BROWSER_PORT=${ANNOTATIONS_BROWSER_PORT}
    command: >
      python -m uvicorn webapps.annotations_browser.app:app
      --host 0.0.0.0
      --port ${ANNOTATIONS_BROWSER_PORT}
      --log-level info
    volumes:
      - ../src/:/app
      - resources:${IMAGE_VOLUME_PATH}
      - ../configs:/app/configs
    depends_on:
      - image-acquisition-service
    networks:
      - ml-network

volumes:
  test:
    external: true
  mlflow:
    name: mlflow
    driver: local
  mlruns:
    name: mlruns
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${LOCAL_MLRUNS_PATH}
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  model_caches:
    name: model_caches
    driver: local
  resources:
    name: resources
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${LOCAL_RESOURCES_PATH}

networks:
  ml-network:
    driver: bridge