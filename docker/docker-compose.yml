name: ${NAME}
services:
  experiment-service:
    build:
      context: experiment-service
      dockerfile: Dockerfile
    container_name: experiment-service
    # Container laufen lassen und auf Eingaben warten, auch wenn kein Terminal verbunden ist
    stdin_open: true
    tty: true
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
      - IMAGE_VOLUME_PATH=/app/volumes/resources
      - IMAGE_ACQUISITION_SERVICE_URL=http://image-acquisition-service:${IMAGE_ACQUISITION_PORT}
      - JUROR_SERVICE_URL=http://juror-service:${JUROR_PORT}
    volumes:
      - ../src/:/app
      - resources:/app/volumes/resources
      - ../configs:/app/configs
      - mlruns:/mlruns
    depends_on:
      - prometheus
      - mlflow
      - image-acquisition-service
      - juror-service
    networks:
      - ml-network

  image-acquisition-service:
    build:
      context: image-acquisition-service
      dockerfile: Dockerfile
    container_name: image-acquisition-service
    ports:
      - "${IMAGE_ACQUISITION_PORT}:${IMAGE_ACQUISITION_PORT}"
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
      - IMAGE_VOLUME_PATH=/app/volumes/resources
    command: >
      python -m uvicorn image_acquisition.acquisition_server.AcquisitionServer:app
      --host 0.0.0.0
      --port ${IMAGE_ACQUISITION_PORT}
      --log-level info
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:${IMAGE_ACQUISITION_PORT}/health || exit 1" ]
      interval: 30s
      timeout: 5s
      retries: 3
    volumes:
      - resources:/app/volumes/resources
      - ../src/:/app
      - ../configs:/app/configs
    depends_on:
      - prometheus
    networks:
      - ml-network

  juror-service:
    build:
      context: juror-service
      dockerfile: Dockerfile
    container_name: juror-service
    ports:
      - "${JUROR_PORT}:${JUROR_PORT}"
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app
    command: >
      python -m uvicorn juror_server.JurorServer:app
      --host 0.0.0.0
      --port ${JUROR_PORT}
      --log-level info
    volumes:
      - ../src/:/app
    depends_on:
      - prometheus
    networks:
      - ml-network
    # deploy block removed: moved to docker/docker-compose.windows.override.yml
    # If you want to enable the GPU reservation (Windows), start compose with:
    # docker-compose -f docker-compose.yml -f docker/docker-compose.windows.override.yml up

  train:
    build:
      context: train
      dockerfile: Dockerfile
    container_name: train
    environment:
      - ENV_NAME=docker
      - PYTHONPATH=/app/src
      - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
      - IMAGE_VOLUME_PATH=/app/volumes/resources
      - IMAGE_ACQUISITION_SERVICE_URL=http://image-acquisition-service:${IMAGE_ACQUISITION_PORT}
      - JUROR_SERVICE_URL=http://juror-service:${JUROR_PORT}
      - MLFLOW_ARTIFACT_ROOT=${MLFLOW_ARTIFACT_ROOT}
    volumes:
      - resources:/app/volumes/resources
      - ../src:/app/src
      - ../configs:/app/configs
      - ../test:/app/test
      - mlruns:/mlruns
    depends_on:
      - mlflow
    networks:
      - ml-network

  mlflow:
    build:
      context: mlflow
      dockerfile: Dockerfile
    container_name: mlflow
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    volumes:
      # Artefakte (Runs) persistent speichern
      - mlruns:/mlruns
      # Backend-DB persistent speichern
      - mlflow:/mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port ${MLFLOW_PORT}
      --backend-store-uri ${MLFLOW_BACKEND_STORE_URI}
      --default-artifact-root ${MLFLOW_ARTIFACT_ROOT}
      --allowed-hosts "*"
      --serve-artifacts
    networks:
      - ml-network

  prometheus:
    build:
      context: prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    ports:
      - "${MONITORING_PROMETHEUS_PORT}:${MONITORING_PROMETHEUS_PORT}"
    environment:
      IMAGE_ACQUISITION_PORT: "${IMAGE_ACQUISITION_PORT}"
      JUROR_PORT: "${JUROR_PORT}"
      MLFLOW_PORT: "${MLFLOW_PORT}"
      MONITORING_PROMETHEUS_PORT: "${MONITORING_PROMETHEUS_PORT}"
      MONITORING_PROMETHEUS_GPU_EXPORTER_PORT: "${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}"
    volumes:
      - prometheus_data:/prometheus
      - ./prometheus/prometheus.yml.template:/etc/prometheus/prometheus.yml.template:ro
    networks:
      - ml-network

  grafana:
    build:
      context: grafana
      dockerfile: Dockerfile
    container_name: grafana
    ports:
      - "${MONITORING_GRAFANA_PORT}:${MONITORING_GRAFANA_PORT}"
    environment:
      MONITORING_GRAFANA_PORT: "${MONITORING_GRAFANA_PORT}"
      GF_SERVER_HTTP_PORT: "${MONITORING_GRAFANA_PORT}"
      GF_SERVER_ROOT_URL: "http://localhost:${MONITORING_GRAFANA_PORT}"
      GF_USERS_ALLOW_SIGN_UP: "false"
      PROMETHEUS_URL: "http://prometheus:${MONITORING_PROMETHEUS_PORT}"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - ml-network
    depends_on:
      - prometheus

  system-gpu-exporter:
    build:
      context: ./exporter
      dockerfile: Dockerfile
    container_name: system-gpu-exporter
    ports:
      - "${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}:${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}"
    environment:
      - ENV_NAME=docker
      - SCRAPE_INTERVAL_S=5
      - EXPORTER_PORT=8000
    volumes:
      - ../src/:/app
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:${MONITORING_PROMETHEUS_GPU_EXPORTER_PORT}/metrics || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
    # If you want to expose host GPUs to this container (NVIDIA), ensure the host has NVIDIA Container Toolkit
    # and your Docker/Compose supports device_requests. Uncomment device_requests to request all gpus.
    #device_requests:
    #  - driver: "nvidia"
    #    count: -1
    #    capabilities: ["gpu"]
    networks:
      - ml-network

volumes:
  test:
    external: true
  mlflow:
    name: mlflow
    driver: local
  mlruns:
    name: mlruns
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  resources:
    name: resources
    driver: local

networks:
  ml-network:
    driver: bridge